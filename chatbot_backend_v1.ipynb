{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "from flask import Flask, render_template, request\n",
    "from keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/varunprakash/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/varunprakash/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/varunprakash/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_words = [\"?\", \"!\", \",\",'.']\n",
    "def clean_up_sentence(sentence):\n",
    "    sentence_words = word_tokenize(sentence)\n",
    "    sentence_words = [lemmatizer.lemmatize(w.lower()) for w in sentence_words if w not in ignore_words]\n",
    "    return sentence_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_input(description):\n",
    "    tokenizer = Tokenizer(num_words=5000)\n",
    "    tokenizer.fit_on_texts(description)\n",
    "    x_description = tokenizer.texts_to_sequences(description)\n",
    "    return x_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_preprocessing(desc):\n",
    "    corpus = []\n",
    "    lem = WordNetLemmatizer()\n",
    "\n",
    "    words = [w for w in nltk.word_tokenize(desc) if (w not in stop)]\n",
    "    words = [lem.lemmatize(w) for w in words if len(w)>2]\n",
    "    words = [''.join(c for c in s if c not in string.punctuation) for s in words if s]\n",
    "    words = [word.lower() for word in words]\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    corpus.append(words) \n",
    "\n",
    "    text = []\n",
    "\n",
    "    for i in range(len(corpus)):\n",
    "        text.append(' '.join(w for w in corpus[i]))\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_accident_level(desc):\n",
    "    maxlen = 56\n",
    "    preprocess = nlp_preprocessing(desc)\n",
    "    model_input = convert_input(preprocess)\n",
    "    x = pad_sequences(model_input, padding='post', maxlen=maxlen)\n",
    "    model = load_model(r\"/Users/varunprakash/Downloads/finalized_keras_model_1.h5\")\n",
    "    resp = model.predict(x)\n",
    "    return np.argmax(resp)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_potential_accident_level(desc):\n",
    "    maxlen = 86\n",
    "    preprocess = nlp_preprocessing(desc)\n",
    "    model_input = convert_input(preprocess)\n",
    "    x = pad_sequences(model_input, padding='post', maxlen=maxlen)\n",
    "    model = load_model('/Users/varunprakash/Downloads/finalized_keras_model_with_target_combine_acc_level.h5')\n",
    "    resp = model.predict(x)\n",
    "    return np.argmax(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [09/Jan/2022 12:04:49] \"\u001b[37mPOST /bot HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [09/Jan/2022 12:05:45] \"\u001b[37mPOST /bot HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'want', 'to', 'report', 'an', 'accident']\n",
      "['at', 'approximately', '6:20', 'p.m.', 'on', '08/06/16', 'the', 'operator', 'eustaquio', 'fall', 'down', 'from', 'the', 'metal', 'platform', 'that', 'give', 'access', 'to', 'tank', 'd-1021', 'of', 'the', 'strong', 'acid', 'leaching', 'stage', 'and', 'suffers', 'luxo-fractures', 'in', 'the', 'wrist', 'left', 'when', 'leaning', 'on', 'the', 'floor', 'with', 'his', 'hand', 'the', 'operator', 'wa', 'directed', 'to', 'the', 'first', 'tank', 'of', 'the', 'strong', 'acid', 'leaching', 'stage', '(', 'tk', 'd-2040', ')', 'to', 'verify', 'the', 'entry', 'of', 'spent', 'to', 'the', 'taque']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [09/Jan/2022 12:06:41] \"\u001b[37mPOST /bot HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accident_level is 3\n",
      "potential_accident_level is 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [09/Jan/2022 12:06:49] \"\u001b[37mPOST /bot HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thanks']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [09/Jan/2022 12:07:38] \"\u001b[37mPOST /bot HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [09/Jan/2022 12:08:51] \"\u001b[37mPOST /bot HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'want', 'to', 'know', 'the', 'acci', 'dmaeafdsf']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [09/Jan/2022 12:13:47] \"\u001b[37mPOST /bot HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sdfsdf']\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return render_template(\"index.html\")\n",
    "\n",
    "\n",
    "@app.route(\"/bot\", methods=[\"POST\"])\n",
    "def bot_rules():\n",
    "    \n",
    "    f = open(r\"/Users/varunprakash/Downloads/intents_new.json\")\n",
    "    data = json.load(f)\n",
    "    \n",
    "    msg = request.form[\"msg\"]\n",
    "    \n",
    "    is_name = False\n",
    "    name = ''\n",
    "    \n",
    "    if msg.startswith('My name is'):\n",
    "        name = msg[11:]\n",
    "        is_name = True\n",
    "\n",
    "    elif msg.startswith('This is'):\n",
    "        name = msg[8:]\n",
    "        is_name = True\n",
    "    \n",
    "    greetings = data['greetings']\n",
    "    greetings_response = data['greetings_response']\n",
    "    \n",
    "    goodbye = data['goodbye']\n",
    "    goodbye_response = data['goodbye_response']\n",
    "\n",
    "    thanks = data['thanks']\n",
    "    thanks_response = data['thanks_response']\n",
    "    \n",
    "    accident = data['accident']\n",
    "    accident_response = data['accident_response']\n",
    "    \n",
    "    default_response = data['default_response']\n",
    "    \n",
    "    text = clean_up_sentence(msg)\n",
    "    print(text)\n",
    "\n",
    "    if is_name:\n",
    "        return \"Hi {name}, how can i help you\".format(name=name)\n",
    "    \n",
    "    if any(x in text for x in greetings):\n",
    "        if is_name:\n",
    "            return greetings_response_with_name\n",
    "        else:\n",
    "            return greetings_response\n",
    "        \n",
    "    elif any(x in text for x in goodbye):\n",
    "        return goodbye_response\n",
    "    \n",
    "    elif any(x in text for x in thanks):\n",
    "        return thanks_response\n",
    "    \n",
    "    elif any(x in text for x in accident):\n",
    "        return accident_response\n",
    "            \n",
    "    elif any(x in msg for x in goodbye):\n",
    "        return goodbye_response\n",
    "    \n",
    "    elif any(x in msg for x in thanks):\n",
    "        return thanks_response\n",
    "    \n",
    "    elif any(x in msg for x in accident):\n",
    "        return accident_response    \n",
    "\n",
    "    elif len(msg) < 35:\n",
    "        return default_response\n",
    "            \n",
    "    else:\n",
    "        accident_level = predict_accident_level(msg)\n",
    "        potential_accident_level = predict_potential_accident_level(msg)\n",
    "        print('accident_level is', accident_level)\n",
    "        print('potential_accident_level is', accident_level)\n",
    "        return data[str(accident_level)] + ' and the Potential accident level is '+ str(potential_accident_level)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
